## 2.1 `unicode1`

**1. What Unicode character does chr(0) return?**

`'\x00'`

**2. How does this character's string representation (__repr__()) differ from its printed representation?**

When printed, it is an empty character.

**3. What happens when this character occurs in text?**

When printed, it behaves as an empty character. But when you look at the string, you can see the added bytes.

---

## 2.2 `unicode2`

**1. What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than UTF-16 or UTF-32?**

Because in UTF-16 and UTF-32, you always need at least 2 and 4 bytes to represent one character, respectively. However, this is a waste of space and sequence, as most characters can be represented with 1 byte, as UTF-8 does and only need multiple bytes when representing larger numbers.

**2. Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into a Unicode string. Why is this function incorrect? Provide an example of an input byte string that yields incorrect results.**

```python
def decode_utf8_bytes_to_str_wrong(bytestring: bytes):
    return "".join([bytes([b]).decode("utf-8") for b in bytestring])
```

This code assumes every character is represented by 1 byte, and this assumption would fail fairly quickly when it comes across any non-ascii characters.

Example: If you provide "uğur" as an input, you get a `UnicodeDecodeError` exception:
```
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc4 in position 0: unexpected end of data
```

**3. Give a two byte sequence that does not decode to any Unicode character(s).**

Not all byte sequences are valid. One such example is to have a continuation byte at the start of a character, e.g. `10111111`. So, for example `10111111 11001111` would not decode to any characters.

Test:
```python
c =  0b1011111111001111
c.to_bytes(2).decode("utf-8")
```

This will raise:
```
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xbf in position 0: invalid start byte
```

---

## 2.5 `train_bpe_tinystories`

**1. Train a byte-level BPE tokenizer on the TinyStories dataset, using a maximum vocabulary size of 10,000. How many hours and memory did training take? What is the longest token in the vocabulary? Does it make sense?**

It used around 2 GBs of peak memory when actively running. For memory, main bottleneck was reading the chunks and keeping them in memory. It takes around 2-2.5 minutes to complete the training.

The longest token in the vocabulary is `b' accomplishment'`, at index `7160` (`max(vocab.items(), key=lambda x: len(x[1]))`). It makes sense, and from the 6904th merge you can see it is the combination of `(b' accomplish', b'ment')`.

**2. Profile your code. What part of the tokenizer training process takes the most time?**

Most memory is used in the pretokenization part, as well as the most time. The memory usage peaks when reading the text chunks from the disk, and also when splitting and rejoining the text on special tokens, whereas the most processing time is spent on regex pattern matching (29% of the running time). The actual BPE training time is 8% of the training time.

![Scalene Results](scalene_results.png)

---

## 2.5 `train_bpe_expts_owt`

**1. Train a byte-level BPE tokenizer on the OpenWebText dataset, using a maximum vocabulary size of 32,000. What is the longest token in the vocabulary? Does it make sense?**

Longest token is at index `25822`, with a value of `b'\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82'` or `'ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ'` when decoded with UTF-8. This seems to be double-encoded UTF-8, an artifact from web scraping.

**2. Compare and contrast the tokenizer that you get training on TinyStories versus OpenWebText.**

Even though OpenWebText tried to exclude non-English documents, we clearly got some artifacts introduced by it being real data. BPE trained on it also learned interesting artifacts. As OWT tokenizer has a larger and presumably more diverse vocabulary, I would expect it to be more efficient in turning byte sequences into token IDs. TinyStories tokenizer on the other hand should have cleaner text, but less diverse outputs as stories were generated by GPT-4, which has a certain linguistic output pattern to it.

---

## 2.7 `tokenizer_experiments`

**1. What is each tokenizer's compression ratio (bytes/token)?**

- OWT Tokenizer, Compression Ratio: ~4.50 bytes/token
- TinyStories Tokenizer, Compression Ratio: ~4.01 bytes/token

**2. What happens if you tokenize your OpenWebText sample with the TinyStories tokenizer? Compare the compression ratio and/or qualitatively describe what happens.**

- OWT Tokenizer on TinyStories samples, Compression Ratio: ~3.87 bytes/token
- TinyStories Tokenizer on OWT samples, Compression Ratio: ~3.40 bytes/token

**3. Estimate the throughput of your tokenizer (e.g., in bytes/second). How long would it take to tokenize the Pile dataset (825GB of text)?**

Input processing speed is highly dependent on the vocabulary size (and the number of merges).

- OWT Input Throughput: ~713258.83 bytes/second
- TinyStories Input Throughput: ~697562.09 bytes/second
- Pile Dataset Processing for OWT Tokenizer: 825e+9 / 713258.83 = ~14 days
- Pile Dataset Processing for TinyStories Tokenizer: 825e+9 / 697562.09 = ~14 days

**4. Using your TinyStories and OpenWebText tokenizers, encode the respective training and development datasets into a sequence of integer token IDs. We'll use this later to train our language model. We recommend serializing the token IDs as a NumPy array of datatype uint16. Why is uint16 an appropriate choice?**

uint8 has a maximum value of 255, and uint16 has a maximum value of 65535, which can represent each of the 32k tokens in the vocabulary. Having the data type unsigned signals these are supposed to be positive numbers, and potentially make it more extensible, i.e. for continued pretraining. We do not want to waste the negative 32k range.

---

## 3.6 `transformer_accounting`

**1. Consider GPT-2 XL, which has the following configuration:**

```
vocab_size : 50,257
context_length : 1,024
num_layers : 48
d_model : 1,600
num_heads : 25
d_ff : 6,400
```

**Suppose we constructed our model using this configuration. How many trainable parameters would our model have? Assuming each parameter is represented using single-precision floating point, how much memory is required to just load this model?**

Parameter breakdown:
- Embeddings: vocab_size x d_model
- Transformer Layers: num_layers x
  - RMSNorm(s): 2 x d_model
  - Attention (assuming d_k x num_heads = d_model): 3 x d_model x d_model + d_model x d_model = 4 x d_model^2
  - Feedforward: 3 x d_model x d_ff
- Final RMSNorm: d_model
- Output Projection: vocab_size x d_model

Total Parameters to Load: (2 x vocab_size x d_model) + num_layers x (2 x d_model + 4 x d_model x d_model + 3 x d_model x d_ff) + d_model = 2,127,056,000

Storing them in single precision means each parameter will need 4 bytes, hence this would take ~7.92 GiB or ~8.51 GB.


**2. Identify the matrix multiplies required to complete a forward pass of our GPT-2 XL-shaped model. How many FLOPs do these matrix multiplies require in total? Assume that our input sequence has context_length tokens.**

Calculation:

**For 1 Transformer Block:**
- 2 RMS → Negligible
- 1 MHA
- 1 FF

**For 1 FF:**
```
2 x batch_size x (seq_len x d_model x d_ff)
2 x batch_size x (seq_len x d_ff x d_model)
2 x batch_size x (seq_len x d_model x d_ff)

= 6 x batch_size x seq_len x d_model x d_ff
```

**For 1 MHA (with RoPE):**
```
2 x 3 x batch_size x (seq_len x d_model x num_heads x d_k)  # From q,k,v projection
2 x 2 x batch_size x seq_len x d_model → Negligible  # From RoPE on k, q
2 x batch_size x seq_len x num_heads x d_k x seq_len  # Dot product between k and q
2 x batch_size x seq_len x seq_len x num_heads x d_v  # weighted sum
2 x batch_size x (seq_len x num_heads x d_v x d_model)  # final projection
```

Assume d_model = num_heads x d_k and d_k = d_v

Then:
```
6 x b x s x d x d
4 x b x s x d → Negligible
2 x b x s x d x s
2 x b x s x s x d
2 x b x s x d x d

= 8 x bsd² + 4 x bs²d
```

**For 1 RMSNorm:** No matrix multiply, negligible

**So, 1 transformer block:**
```
(6 x b x seq x d_model x d_ff) + (8 x b x seq x d_model x d_model) + (4 x b x seq x seq x d)
```

**Then, for a Transformer LM, forward pass costs:**
- Embedding: None (Look-Up)
- Transformer Blocks: num_layers x (6bsdd_ff + 8bsd² + 4bs²d)
- RMS: Negligible
- Out Projection: 2bsdv

**Final calculation:**
- Transformer Blocks: 48 x (6 x 1024 x 1600 x 6400 + 8 x 1024 x 1600 x 1600 + 4 x 1024 x 1024 x 1600) = 4.3486544e+12
- Output Projection: 2 x 1024 x 1600 x 50257 = 0.164682137600e+12
- **Total: ~4.5 TFLOPs**

**3. Based on your analysis above, which parts of the model require the most FLOPs?**

Most FLOPs are required for the dot product between keys and queries in the attention. Weighted sum of attention can also be costly depending on if context length is significant compared to the model dimension. One interesting factor we can notice here is that we didn't actually need to assume n_heads x d_k was equal to d_model but we just assumed by convention. So, one can play around with the number of heads. And in the SwiGLU case, feedforward network is also spending a lot of FLOPs. Output projection also takes a lot of FLOPs relative to what it is doing, and scales by the size of the vocabulary.

**4. Repeat the analysis for GPT-2 Small, Medium and Large. As the model size increases, which part of the model take proportionally more or less of total FLOPs?**

```
================================================ GPT-2 Small ================================================

Total FLOPs: 3.50E+11
Output Projection FLOPs: 7.90E+10 | Relative Contribution to the total FLOPs: 22.61%
Transformer Blocks FLOPs: 2.705829E+11 | Relative Contribution to the total FLOPs: 77.39%

=====

Single Transformer Block FLOPs: 2.25E+10
Attention FLOPs: 8.05E+09 | Relative Contribution to the transformer block FLOPs: 35.71%
Feedforward FLOPs: 1.45E+10 | Relative Contribution to the transformer block FLOPs: 64.29%


================================================ GPT-2 Medium ================================================

Total FLOPs: 1.03E+12
Output Projection FLOPs: 1.05E+11 | Relative Contribution to the total FLOPs: 10.20%
Transformer Blocks FLOPs: 9.277129E+11 | Relative Contribution to the total FLOPs: 89.80%

=====

Single Transformer Block FLOPs: 3.87E+10
Attention FLOPs: 1.29E+10 | Relative Contribution to the transformer block FLOPs: 33.33%
Feedforward FLOPs: 2.58E+10 | Relative Contribution to the transformer block FLOPs: 66.67%


================================================ GPT-2 Large ================================================

Total FLOPs: 2.26E+12
Output Projection FLOPs: 1.32E+11 | Relative Contribution to the total FLOPs: 5.84%
Transformer Blocks FLOPs: 2.126009E+12 | Relative Contribution to the total FLOPs: 94.16%

=====

Single Transformer Block FLOPs: 5.91E+10
Attention FLOPs: 1.88E+10 | Relative Contribution to the transformer block FLOPs: 31.82%
Feedforward FLOPs: 4.03E+10 | Relative Contribution to the transformer block FLOPs: 68.18%


================================================ GPT-2 XL ================================================

Total FLOPs: 4.51E+12
Output Projection FLOPs: 1.65E+11 | Relative Contribution to the total FLOPs: 3.65%
Transformer Blocks FLOPs: 4.348654E+12 | Relative Contribution to the total FLOPs: 96.35%

=====

Single Transformer Block FLOPs: 9.06E+10
Attention FLOPs: 2.77E+10 | Relative Contribution to the transformer block FLOPs: 30.56%
Feedforward FLOPs: 6.29E+10 | Relative Contribution to the transformer block FLOPs: 69.44%
```

(Code can be found under cs336_basics/flops_calculator.py)

As the model size increases, FLOPs contribution of the output projection layer gets smaller and smaller. Additionally, inside the transformer block itself we see proportional contribution of the attention layers getting slightly smaller as well while contribution of the feedforward components increase, as well as the contribution of the transformer blocks to the total model FLOPs.

**5. Increase context length of the GPT-2 XL to 16384. How do the relative contributions change?**

When we increase the context length, this is what we observe:

```
Total FLOPs: 1.50E+14
Output Projection FLOPs: 2.63E+12 | Relative Contribution to the total FLOPs: 1.76%
Transformer Blocks FLOPs: 1.468879E+14 | Relative Contribution to the total FLOPs: 98.24%

=====

Single Transformer Block FLOPs: 3.06E+12
Attention FLOPs: 2.05E+12 | Relative Contribution to the transformer block FLOPs: 67.11%
Feedforward FLOPs: 1.01E+12 | Relative Contribution to the transformer block FLOPs: 32.89%
```

Attention blocks start to dominate (compared to before when feedforwards were dominating).

---

## 4.2 `learning_rate_tuning`

**1. Run the SGD example above with three other values for the learning rate: 1e1, 1e2, and 1e3, for just 10 training iterations. What happens with the loss for each of these learning rates? Does it decay faster, slower, or does it diverge (i.e., increase over the course of training)?**

For learning rates 1e1 and 1e2, loss decays faster in a correlated manner with the magnitude of the learning rate. For 1e3, loss diverges.

---

## 4.3 `adamwAccounting`

**1. How much peak memory does running AdamW require? Decompose your answer based on the memory usage of the parameters, activations, gradients, and optimizer state.**

AdamW stores extra states for each parameter that has a gradient. Namely, it stores the first and second moment vectors of the same size with the parameter itself.

**Parameters:**

Previously, we have calculated the needed memory to load a model into memory as (2 x vocab_size x d_model) + num_layers x (2 x d_model + 4 x d_model x d_model + 3 x d_model x d_ff) + d_model. Though, because there I assumed we were using SwiGLU, and this question assumes only W1 and W2 in the feedforward, this equation should be updated as:

```
(2 x vocab_size x d_model) + num_layers x (2 x d_model + 4 x d_model x d_model + 2 x d_model x d_ff) + d_model
```

**Gradients:**

After backpropagation, all of these parameters will have calculated gradients of the same size. So, we have another:

```
(2 x vocab_size x d_model) + num_layers x (2 x d_model + 4 x d_model x d_model + 2 x d_model x d_ff) + d_model
```

**Optimizer State:**

Bookkeeping of AdamW means we would need to store two state vectors, namely first and the second moment vectors for each parameter with a gradient. This means AdamW optimizer state will cost us double the memory usage of the parameters:

```
(4 x vocab_size x d_model) + 2 x num_layers x (2 x d_model + 4 x d_model x d_model + 2 x d_model x d_ff) + 2 x d_model
```

**Activations:**

Finally, we can calculate the memory needed for the forward propagation, i.e. activations. Let's assume our input is of shape (b, s, d), where b = batch size, s = sequence length and d = d_model. And for simplicity, I will assume d_model = num_heads x d_attn.

*Embedding:*
- Basically, it does not have any activations. It is just a look-up operation.

*Transformer Block (single):*

1. Input copy for residual stream: b x s x d
2. RMSNorm intermediate mean operation: b x s, and output: b x s x d
3. Projection to q, k, v (three linear operations, store outputs): 3 x b x s x d
4. Attention scores calculation (matrix multiply keys and queries, then softmax, per head): 2 x b x s x s x h (where h = num_heads)
5. Weighted sum of value vectors: b x s x d
6. Final linear projection: b x s x d

Summing for attention: bsd + bsd + bs + 3bsd + 2bs²h + bsd + bsd = 7bsd + 2bs²h

*Feed-Forward Network:*

1. Input copy for residual stream: bsd
2. RMSNorm: bs + bsd
3. First mapping (b, s, d) to (b, s, d_ff): bsd_ff
4. SiLU activation: bsd_ff
5. Mapping back to d: bsd

Summing for feedforward: bs + bsd + bsd + bsd_ff + bsd_ff + bsd = bs + 3bsd + 2bsd_ff

*Total per transformer block:* 2bs + 10bsd + 2bs²h + 2bsd_ff

*All transformer blocks:* n x (2bs + 10bsd + 2bs²h + 2bsd_ff)

*Final RMSNorm:* bs + bsd

*Output embedding (no weight-tying):* bsv (where v is the vocabulary size)

*Cross entropy (non-fused kernels):* bsv

**Total activations at peak:** n x (2bs + 10bsd + 2bs²h + 2bsd_ff) + bs + bsd + 2bsv

**Summary:**

- **Parameters Memory Usage:** 2vd + n(2d + 4d² + 2dd_ff) + d
- **Activations Memory Usage:** n x (2bs + 10bsd + 2bs²h + 2bsd_ff) + bs + bsd + 2bsv
- **Gradients Memory Usage:** 2vd + n(2d + 4d² + 2dd_ff) + d
- **Optimizer State Memory Usage:** 4vd + 2n(2d + 4d² + 2dd_ff) + 2d
- **Whole model:** 8vd + 4n(2d + 4d² + 2dd_ff) + 4d + n(2bs + 10bsd + 2bs²h + 2bsd_ff) + bs + bsd + 2bsv

**2. Instantiate your answer for a GPT-2 XL-shaped model to get an expression that only depends on the batch_size. What is the maximum batch size you can use and still fit within 80GB memory? (Assuming fp32)**

Substituting GPT-2 XL parameters:

```
643,289,600 + 5,898,854,400 + 6400 + 48b(2048 + 16,384,000 + 52,428,800 + 13,107,200) + b(1024 + 1,638,400 + 102,926,336)
= 6,542,150,400 + b(4,036,824,064)
```

Assuming fp32, this model would use **26.17 + 16.15 x batch_size GB** memory.

Maximum batch size that would fit in 80 GB: **3** (calculated as floor((80 - 26.17) / 16.15))

**3. How many FLOPs does running one step of AdamW take?**

Breaking down the AdamW update step (where P = number of parameters):

**First moment vector calculation:**
- Scale parameters by scalar: P
- Scale gradients: P
- Sum the two: P
- Total: 3P FLOPs

**Second moment vector calculation:**
- Square gradients (elementwise): P
- Scale by scalar: P
- Scale second moment: P
- Sum: P
- Total: 4P FLOPs

**Parameter update:**
- Square root of second moment: P
- Add epsilon: P
- Divide first moment by (sqrt(second moment) + epsilon): P
- Multiply by learning rate: P
- Subtract from parameters: P
- Total: 5P FLOPs

**Weight decay:**
- Elementwise multiplication: P
- Subtraction: P
- Total: 2P FLOPs

**Total: 3P + 4P + 5P + 2P = 14P FLOPs** (where P is the number of parameters)

**4. An NVIDIA A100 GPU has a theoretical peak of 19.5 teraFLOP/s for float32 operations. Assuming you are able to get 50% MFU, how long would it take to train a GPT-2 XL for 400K steps and a batch size of 1024 on a single A100?**

Following Kaplan et al., whose analyses were made on GPT-2 like architectures, we can assume 1 forward + backward pass will take **6PBS FLOPs**, where:
- Forward pass: 2PBS
- Backward pass: 4PBS
- P = Number of parameters
- B = Batch size
- S = Sequence length

**GPT-2 XL parameter count:**

```
(2 x vocab_size x d_model) + num_layers x (2 x d_model + 4 x d_model x d_model + 2 x d_model x d_ff) + d_model
= (160,822,400 + 1,474,713,600 + 1,600)
= 1,635,537,600 parameters
```

**FLOPs per step (batch size 1024):**

```
6 x 1,635,537,600 x 1024 x 1024 = 10,289,912,846,745,600 FLOPs
≈ 10,290 teraFLOPs
```

**Training time:**

- A100 theoretical peak: 19.5 teraFLOP/s
- With 50% MFU: 9.75 teraFLOP/s
- Time per step: 10,290 / 9.75 ≈ 1,055 seconds
- Total for 400K steps: 422,153,846 seconds
- **≈ 13 years and 4.5 months**

---

## 7.1 `experiment_log`

All experiments can be found here:
- Learning rate tuning: https://wandb.ai/ugurkap/cs336-a1-tuning?nw=nwuserugurkap
- Batch size experiments: https://wandb.ai/ugurkap/cs336-a1-batchsize?nw=nwuserugurkap
- Ablations: https://wandb.ai/ugurkap/cs336-a1-ablations?nw=nwuserugurkap
- OWT Training: https://wandb.ai/ugurkap/cs336-a1-owt?nw=nwuserugurkap

---

## 7.2

### `learning_rate`

**1. Explain your hyperparameter search strategy.**

I did telescopic search. Basically, tested different learning rates at different scales, e.g. 1e-1, 1e-2, 1e-3, and then once I found the best performing ones, I searched in between. Did somewhat of a light "binary search". I trained a model with a validation loss of 1.32.

**2. Investigate how the point at which learning rates diverge is related to your best learning rate.**

For me, that was not the case. Potentially, because I used beta1 = beta2 for AdamW, which is known to stabilize training, as well as the usual advice of using 1% of the iterations for warm-up and rest for the cosine decay down to 10% of the maximum learning rate. These additional stabilizing factors might be masking where the "edge of stability" is, or maybe this is not the case for a smallish model with only 17M parameters.

### `batch_size_experiment`

**1. A few sentences discussing of your findings on batch sizes and their impacts on training.**

I had the best performance with (surprisingly) batch size 64, but the difference is not significant. What is more interesting is, with the largest batch size possible (1024), it was much harder to tune the learning rate, so using something smallish was a better idea. Furthermore, we observe a correlative relationship between the maximum learning rate and the batch size. As the batch size grows, higher learning rates result in better performance. For example, (b=64, lr=1e-3) performs as good as (b=512, lr=1e-2), whereas (b=256, lr=1e-2) is outperformed by both of them. This can be probably explaine by the noise introduced by smaller batch sizes. Less noise in the samples, i.e. larger batch sizes, are stabilizing factors that can let us increase the learning rate.

### `generate`

**1. Using your decoder and your trained checkpoint, report the text generated by your model.**

This is the generated text with temperature=0.5, top-p=0.8:

> Once upon a time, there was a little boy named Tim. Tim loved to play outside. One day, he saw a big, red ball in the park. He wanted to play with it. Tim ran to the ball and said, "Ball, let's play!" But the ball did not move. Tim felt frustrated. He tried again and again, but the ball still did not move. Tim was sad. Then, a big, friendly dog came to Tim. The dog said, "I can help you!" The dog pushed the ball with its nose. The ball started to move! Tim was so happy. He played with the dog and the ball all day.

---

## 7.3

### `layer_norm_ablation`

**1. Remove all of the RMSNorms from your Transformer and train. What happens at the previous optimal learning rate?**

Training almost diverges (during the high learning rate phase) and then stabilizes (during cosine annealing). Using no normalization ends up being the worst performing compared to all others in the ablation.

### `pre_norm_ablation`

**1. Modify your pre-norm Transformer implementation into a post-norm one. Train with the post-norm model and see what happens.**

Post-Norm actually performs similarly to Pre-Norm transformer, however it is slightly worse.

### `no_pos_emb`

**1. Modify your Transformer implementation with RoPE to remove the position embedding information entirely, and see what happens.**

NoPE performs even worse than post-norm variant, showing the importance of incorporating positional embeddings. However, even in 256 context size, removing RoPE frees around 2 GB VRAM.

### `swiglu_ablation`

**1. Compare the performance of SwiGLU feed-forward networks versus feedforward networks using SiLU activations but no gated linear unit.**

Surprisingly, SiLU performs better than SwiGLU (1.33 vs 1.35 validation loss), however this can be attributed to random noise and working on a small scale.

---

## 7.4 `main_experiment`

**1. Train your language model on OpenWebText with the same model architecture and total training iterations as TinyStories. How well does this model do?**

Optimum (or near optimum) parameters found for the TinyStories dataset seem to perform okay for OWT data as well, surpassing the naive baseline for the leaderboard in 25 minutes. However, at the end of 5000 iterations (after seeing ~327M tokens), it still has a high perplexity around 50.17, which means model is mostly choosing / staying indecisive about 50 tokens for each prediction. The loss curve seems promising, and maybe with a continued pretraining it can perform better, however as it stands model has still not learned the small details and intricasies of the language. This can also be seen by the generated text (starting prompt: Football, temperature: 0.1 top-p: 1.0):

> Football or football football's players, making sure everything is up to date. This is simply done by using microwave tickets or the radio, but it is impossible to carry more than 500 dishes worth to build.
>
> The idea is that it is a passive transervice to New Orleans.
>
> Next is a USC 0-88 Seas strike scenario for more athletic coaches that are often trying to take the ball down solving the offense. The quarterback basically is a full-time pirate and will easily take the ball down the field under the toughest circumstances to overcome missing ground. Regardless of who is — a ship, a football, no regulator. Instead, Army officials would likely cap their fees forbable change. So I say it's normal to have people in the room that get to play 90 minutes on the field in order to claim their votes for the remaining three then have to focus on game time. Then, in practice, waiting for 3-4 Pro Bowl Bryce Brown (the 12-3 Pro Bowl) stop once in the game and it can bet on who is picking the ball anymore.
>
> How does the fourth-person (uclear!) general release feel under the radar, and perhaps start to do more of a picture? Or did that decide just how well the fourth-person squad is? Go! Don't know. And it's more than likely not late, but all eyes will disappoint.
>
> With everything up to speed, it all starts with a 2-5 Pro Bowl. If you are; the base team sloped into the back seats as it kept calming the game. But this has 13 rushing yards and more touchdowns than mostitations that you consciously reach.
>
> See it planned on this final week, along with The Sun's Rogers Brookes Sam Beasack and Michael DeFarlane. Now that we're halfway, and we get to the top six these quarterbacks in RGIII.