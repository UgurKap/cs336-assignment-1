## 2.1 `unicode1`

1. What Unicode character does chr(0) return?  
--> '\x00'

2. How does this character’s string representation (__repr__()) differ from its printed representation?  
--> When printed, it is an empty character.

3. What happens when this character occurs in text?
--> When printed, it behaves as an empty character. But when you look at the string, you can see the added bytes.  

## 2.2 `unicode2`

1. What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than UTF-16 or UTF-32?  
--> Because in UTF-16 and UTF-32, you always need at least 2 and 4 bytes to represent one character, respectively. However, this is a waste of space and sequence, as most characters can be represented with 1 byte, as UTF-8 does and only need multiple bytes when representing larger numbers.

2. Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into a Unicode string. Why is this function incorrect? Provide an example of an input byte string that yields incorrect results.  

```python
def decode_utf8_bytes_to_str_wrong(bytestring: bytes):
    return "".join([bytes([b]).decode("utf-8") for b in bytestring])
```

--> This code assumes every character is represented by 1 byte, and this assumption would fail fairly quickly when it comes across any non-ascii characters. Example: If you provide "uğur" as an input, you get a `UnicodeDecodeError` exception:
`UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc4 in position 0: unexpected end of data`

3. Give a two byte sequence that does not decode to any Unicode character(s).  
--> Not all byte sequences are valid. One such example is to have a continuation byte at the start of a character, e.g. `10111111`. So, for example `10111111 11001111` would not decode to any characters.
Test:
```python
c =  0b1011111111001111
c.to_bytes(2).decode("utf-8")
```
will raise `UnicodeDecodeError: 'utf-8' codec can't decode byte 0xbf in position 0: invalid start byte`

## 2.4 BPE Tokenizer Training

## `train_bpe_tinystories`

1. Train a byte-level BPE tokenizer on the TinyStories dataset, using a maximum vocabulary size of 10,000. How many hours and memory did training take? What is the longest token in the vocabulary? Does it make sense?
--> It used around 2 GBs of peak memory when actively running. For memory, main bottleneck was reading the chunks and keeping them in memory. It takes around 2-2.5 minutes to complete the training. The longest token in the vocabulary is `b' accomplishment'`, at index `7160` (`max(vocab.items(), key=lambda x: len(x[1]))`). It makes sense, and from the 6904th merge you can see it is the combination of `(b' accomplish', b'ment')`.

2. Profile your code. What part of the tokenizer training process takes the most time?
--> Most memory is used in the pretokenization part, as well as the most time. The memory usage peaks when reading the text chunks from the disk, and also when splitting and rejoining the text on special tokens, whereas the most processing time is spent on regex pattern matching (29% of the running time). The actual BPE training time is 8% of the training time.

![Scalene Results](scalene_results.png)

## `train_bpe_expts_owt`

1. Train a byte-level BPE tokenizer on the OpenWebText dataset, using a maximum vocabulary size of 32,000. What is the longest token in the vocabulary? Does it make sense?
--> Longest token is at index `25822`, with a value of `b'\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82'` or `'ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ'` when decoded with UTF-8. This seems to be double-encoded UTF-8, an artifact from web scraping.

2. Compare and contrast the tokenizer that you get training on TinyStories versus OpenWebText.
--> Even though OpenWebText tried to exclude non-English documents, we clearly got some artifacts introduced by it being real data. BPE trained on it also learned interesting artifacts. As OWT tokenizer has a larger and presumably more diverse vocabulary, I would expect it to be more efficient in turning byte sequences into token IDs. TinyStories tokenizer on the other hand should have cleaner text, but less diverse outputs as stories were generated by GPT-4, which has a certain linguistic output pattern to it.